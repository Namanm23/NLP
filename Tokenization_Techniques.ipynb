{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO22VPeXmpp5WdFnEAHkVrx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Namanm23/NLP/blob/main/Tokenization_Techniques.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "klaAkRSIegeF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd42cb0a-d847-4897-8174-b93443f9a2c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Tokenization:\n",
            "['NLP', 'LAB', '1', 'EXP', '1', ':', 'Apply', 'different', 'tokenization', 'techniques', 'using', 'NLTK', '.', 'Learn', 'about', 'the', 'natural', 'language', 'toolkit', '.']\n",
            "\n",
            "Sentence Tokenization:\n",
            "['NLP LAB 1 EXP 1: Apply different tokenization techniques using NLTK.', 'Learn about the natural language toolkit.']\n",
            "\n",
            "RegExp Tokenization (Alphabets Only):\n",
            "['NLP', 'LAB', 'EXP', 'Apply', 'different', 'tokenization', 'techniques', 'using', 'NLTK', 'Learn', 'about', 'the', 'natural', 'language', 'toolkit']\n",
            "\n",
            "RegExp Tokenization (Alphabets and Digits):\n",
            "['NLP', 'LAB', '1', 'EXP', '1', 'Apply', 'different', 'tokenization', 'techniques', 'using', 'NLTK', 'Learn', 'about', 'the', 'natural', 'language', 'toolkit']\n",
            "\n",
            "Custom Tokenization (Splitting by Hyphens):\n",
            "['NLP LAB 1 EXP 1: Apply different tokenization techniques using NLTK. Learn about the natural language toolkit.']\n",
            "\n",
            "Tweet Tokenization:\n",
            "['This', 'is', 'a', '#tweet', 'with', '@mentions', 'and', 'a', 'URL', ':', 'https://example.com']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ],
      "source": [
        "# Install and import necessary libraries\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize, regexp_tokenize, TweetTokenizer\n",
        "\n",
        "# Ensure necessary NLTK resources are downloaded\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab') # Download the punkt_tab data package\n",
        "\n",
        "# Sample text for tokenization\n",
        "sample_text = \"NLP LAB 1 EXP 1: Apply different tokenization techniques using NLTK. Learn about the natural language toolkit.\"\n",
        "\n",
        "# Tokenization functions\n",
        "def word_tokenization(text):\n",
        "    return word_tokenize(text)\n",
        "\n",
        "def sentence_tokenization(text):\n",
        "    return sent_tokenize(text)\n",
        "\n",
        "def regex_tokenization(text, pattern):\n",
        "    return regexp_tokenize(text, pattern=pattern)\n",
        "\n",
        "def custom_tokenization(text, delimiter):\n",
        "    return text.split(delimiter)\n",
        "\n",
        "def tweet_tokenization(tweet):\n",
        "    tweet_tokenizer = TweetTokenizer()\n",
        "    return tweet_tokenizer.tokenize(tweet)\n",
        "\n",
        "# Apply and display tokenization techniques\n",
        "print(\"Word Tokenization:\")\n",
        "print(word_tokenization(sample_text))\n",
        "\n",
        "print(\"\\nSentence Tokenization:\")\n",
        "print(sentence_tokenization(sample_text))\n",
        "\n",
        "print(\"\\nRegExp Tokenization (Alphabets Only):\")\n",
        "print(regex_tokenization(sample_text, r'\\b[a-zA-Z]+\\b'))\n",
        "\n",
        "print(\"\\nRegExp Tokenization (Alphabets and Digits):\")\n",
        "print(regex_tokenization(sample_text, r'\\b\\w+\\b|\\d+'))\n",
        "\n",
        "print(\"\\nCustom Tokenization (Splitting by Hyphens):\")\n",
        "print(custom_tokenization(sample_text, '-'))\n",
        "\n",
        "print(\"\\nTweet Tokenization:\")\n",
        "tweet_text = \"This is a #tweet with @mentions and a URL: https://example.com\"\n",
        "print(tweet_tokenization(tweet_text))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FZE5Ble51kUj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}